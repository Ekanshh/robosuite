{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66544504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# [1] https://www.youtube.com/watch?v=Mut_u40Sqz4\n",
    "# [2] https://github.com/ARISE-Initiative/robosuite/issues/131"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034ffa2",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11dae8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os\n",
    "# openai \n",
    "import gym\n",
    "# Stable baseline imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Robosuite imports\n",
    "import robosuite as suite\n",
    "from robosuite.wrappers import GymWrapper\n",
    "from robosuite.environments.base import register_env\n",
    "from robosuite import load_controller_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3360c",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da35582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how the environment is wrapped by the wrapper\n",
    "env = GymWrapper(\n",
    "    suite.make(\n",
    "                \"TwoArmPegInHole\",\n",
    "                robots=[\"Panda\", \"Panda\"],  # use Sawyer robot\n",
    "                use_camera_obs=False,  # do not use pixel observations\n",
    "                has_offscreen_renderer=False,  # not needed since not using pixel obs\n",
    "                has_renderer=False,  # make sure we can render to the screen\n",
    "                reward_shaping=True,  # use dense rewards\n",
    "                control_freq=20,  # control should happen fast enough so that simulation looks smooth\n",
    "                horizon = 200,   \n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac008b",
   "metadata": {},
   "source": [
    "# Environment Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aff497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial State observations: \\n {env.reset()}\")\n",
    "print(f\"Action space: \\n{env.action_space}\")\n",
    "print(f\"Observation space: \\n{env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02638533",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e4a9b",
   "metadata": {},
   "source": [
    "# Random Action: No Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "for i_episode in range(n_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode: {i_episode} Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242859c",
   "metadata": {},
   "source": [
    "# (NOT WORKING AS EXPECTED) Train using Stable baselines3 PPO without vectorized environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to store training logs\n",
    "\n",
    "\n",
    "# tb_log_dir = os.path.join('../../Training', 'tb_log')\n",
    "# print(tb_log_dir)\n",
    "# monitor_log_dir = os.path.join('../../Training', 'monitor_log')\n",
    "# print(monitor_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4be8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = wrap_env(env)orizon of \n",
    "# Note: n_steps = n * horizon\n",
    "\n",
    "\n",
    "# model = PPO('MlpPolicy', env, n_steps=10, verbose=2, tensorboard_log=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78e8f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=4, tb_log_name=\"TwoArmPegInHole_PPO_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1756f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_path = os.path.join('../Training', 'Saved_Models', 'TwoArmPegInHole_PPO_model')\n",
    "# save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(save_model_path)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c8492",
   "metadata": {},
   "source": [
    "# Interfacing Stable Baseline3 with basic cartpole environment to figure out the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete gym env\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# create PPO model\n",
    "# Note: n_steps here represents the horizon of each episode. For CartPole-v0, the horizon is 200 steps\n",
    "model = PPO(\"MlpPolicy\", env, n_steps=200, verbose=1)\n",
    "model.learn(total_timesteps=400)\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ebd3c",
   "metadata": {},
   "source": [
    "> Note: There is some mis alignment with n_step for robosuite environments. n_steps is not equal to single episode horizon but it is `n` times `single episode horizon in robosuite`. So n_steps = 10 means 10* 200 (i.e. horizon). Somehow we need to find another alternative to get the rollouts not every step but every horizon.\n",
    "\n",
    "> TODO: \n",
    "[] To look into EvalCallback <https://github.com/ludvikka/temp_oj/blob/4c4bf1dde764c0241a7d2080d484937c4584e185/code/rl_training.py#L15>\n",
    "[] To look into robosuite benchmark repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc51886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
